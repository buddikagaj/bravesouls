{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::023375022819:role/service-role/AmazonSageMaker-ExecutionRole-20181029T121824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Created S3 bucket: sagemaker-us-east-1-023375022819\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker-us-east-1-023375022819\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import json\n",
    "import boto3\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()\n",
    "print(role) # This is the role that SageMaker would use to leverage AWS resources (S3, CloudWatch) on your behalf\n",
    "\n",
    "bucket = sess.default_bucket() # Replace with your own bucket name if needed\n",
    "print(bucket)\n",
    "prefix = 'bravesouls/supervised'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data():\n",
    "    !mkdir Data\n",
    "    !aws s3 cp s3://aws-ml-chicago-team-bravesouls/amazon_review_polarity_csv.tgz Data\n",
    "    !tar -xvzf Data/amazon_review_polarity_csv.tgz\n",
    "#prep_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!head -100000 amazon_review_polarity_csv/train.csv > amazon_review_polarity_csv/train_100k.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from random import shuffle\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "import csv\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_instance(row):\n",
    "    cur_row = []\n",
    "    label = \"__label__\" + row[0]  #Prefix the index-ed label with __label__\n",
    "    cur_row.append(label)\n",
    "    cur_row.extend(nltk.word_tokenize(row[2].lower()))\n",
    "    return cur_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(input_file, output_file, keep=1):\n",
    "    all_rows = []\n",
    "    with open(input_file, 'r') as csvinfile:\n",
    "        csv_reader = csv.reader(csvinfile, delimiter=',')\n",
    "        for row in csv_reader:\n",
    "            all_rows.append(row)\n",
    "    shuffle(all_rows)\n",
    "    all_rows = all_rows[:int(keep*len(all_rows))]\n",
    "    pool = Pool(processes=multiprocessing.cpu_count())\n",
    "    transformed_rows = pool.map(transform_instance, all_rows)\n",
    "    pool.close() \n",
    "    pool.join()\n",
    "    \n",
    "    with open(output_file, 'w') as csvoutfile:\n",
    "        csv_writer = csv.writer(csvoutfile, delimiter=' ', lineterminator='\\n')\n",
    "        csv_writer.writerows(transformed_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 46.3 s, sys: 5.83 s, total: 52.1 s\n",
      "Wall time: 4min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Preparing the training dataset\n",
    "\n",
    "# Since preprocessing the whole dataset might take a couple of mintutes,\n",
    "# we keep 20% of the training dataset for this demo.\n",
    "# Set keep to 1 if you want to use the complete dataset\n",
    "preprocess('amazon_review_polarity_csv/train.csv', 'amazon_review_polarity.train', keep=.2)\n",
    "        \n",
    "# Preparing the validation dataset        \n",
    "preprocess('amazon_review_polarity_csv/test.csv', 'amazon_review_polarity.validation', keep=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.32 s, sys: 1.57 s, total: 3.89 s\n",
      "Wall time: 3.79 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_channel = prefix + '/train'\n",
    "validation_channel = prefix + '/validation'\n",
    "\n",
    "sess.upload_data(path='amazon_review_polarity.train', bucket=bucket, key_prefix=train_channel)\n",
    "sess.upload_data(path='amazon_review_polarity.validation', bucket=bucket, key_prefix=validation_channel)\n",
    "\n",
    "s3_train_data = 's3://{}/{}'.format(bucket, train_channel)\n",
    "s3_validation_data = 's3://{}/{}'.format(bucket, validation_channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_output_location = 's3://{}/{}/output'.format(bucket, prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_name = boto3.Session().region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using SageMaker BlazingText container: 811284229777.dkr.ecr.us-east-1.amazonaws.com/blazingtext:latest (us-east-1)\n"
     ]
    }
   ],
   "source": [
    "container = sagemaker.amazon.amazon_estimator.get_image_uri(region_name, \"blazingtext\", \"latest\")\n",
    "print('Using SageMaker BlazingText container: {} ({})'.format(container, region_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bt_model = sagemaker.estimator.Estimator(container,\n",
    "                                         role, \n",
    "                                         base_job_name= \"Gaj-BraveSouls\",\n",
    "                                         train_instance_count=1, \n",
    "                                         train_instance_type='ml.c4.4xlarge',\n",
    "                                         train_volume_size = 30,\n",
    "                                         train_max_run = 360000,\n",
    "                                         input_mode= 'File',\n",
    "                                         output_path=s3_output_location,\n",
    "                                         sagemaker_session=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "bt_model.set_hyperparameters(mode=\"supervised\",\n",
    "                            epochs=10,\n",
    "                            min_count=2,\n",
    "                            learning_rate=0.05,\n",
    "                            vector_dim=10,\n",
    "                            early_stopping=True,\n",
    "                            patience=4,\n",
    "                            min_epochs=5,\n",
    "                            word_ngrams=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = sagemaker.session.s3_input(s3_train_data, distribution='FullyReplicated', \n",
    "                        content_type='text/plain', s3_data_type='S3Prefix')\n",
    "validation_data = sagemaker.session.s3_input(s3_validation_data, distribution='FullyReplicated', \n",
    "                             content_type='text/plain', s3_data_type='S3Prefix')\n",
    "data_channels = {'train': train_data, 'validation': validation_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: Gaj-BraveSouls-2019-01-09-14-46-43-374\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-09 14:46:43 Starting - Starting the training job...\n",
      "2019-01-09 14:46:46 Starting - Launching requested ML instances......\n",
      "2019-01-09 14:47:51 Starting - Preparing the instances for training......\n",
      "2019-01-09 14:49:07 Downloading - Downloading input data\n",
      "2019-01-09 14:49:07 Training - Training image download completed. Training in progress..\n",
      "\u001b[31mArguments: train\u001b[0m\n",
      "\u001b[31m[01/09/2019 14:49:07 WARNING 139969153353536] Loggers have already been setup.\u001b[0m\n",
      "\u001b[31m[01/09/2019 14:49:07 WARNING 139969153353536] Loggers have already been setup.\u001b[0m\n",
      "\u001b[31m[01/09/2019 14:49:07 INFO 139969153353536] nvidia-smi took: 0.0251729488373 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[31m[01/09/2019 14:49:07 INFO 139969153353536] Running single machine CPU BlazingText training using supervised mode.\u001b[0m\n",
      "\u001b[31m[01/09/2019 14:49:07 INFO 139969153353536] Processing /opt/ml/input/data/train/amazon_review_polarity.train . File size: 294 MB\u001b[0m\n",
      "\u001b[31m[01/09/2019 14:49:07 INFO 139969153353536] Processing /opt/ml/input/data/validation/amazon_review_polarity.validation . File size: 32 MB\u001b[0m\n",
      "\u001b[31mRead 10M words\u001b[0m\n",
      "\u001b[31mRead 20M words\u001b[0m\n",
      "\u001b[31mRead 30M words\u001b[0m\n",
      "\u001b[31mRead 40M words\u001b[0m\n",
      "\u001b[31mRead 50M words\u001b[0m\n",
      "\u001b[31mRead 60M words\u001b[0m\n",
      "\u001b[31mRead 62M words\u001b[0m\n",
      "\u001b[31mNumber of words:  210969\u001b[0m\n",
      "\u001b[31mLoading validation data from /opt/ml/input/data/validation/amazon_review_polarity.validation\u001b[0m\n",
      "\u001b[31mLoaded validation data.\u001b[0m\n",
      "\u001b[31m-------------- End of epoch: 1\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0442  Progress: 11.57%  Million Words/sec: 39.62 #####\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0416  Progress: 16.88%  Million Words/sec: 40.18 #####\u001b[0m\n",
      "\u001b[31m-------------- End of epoch: 2\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0389  Progress: 22.24%  Million Words/sec: 40.58 #####\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0362  Progress: 27.59%  Million Words/sec: 40.81 #####\u001b[0m\n",
      "\u001b[31m-------------- End of epoch: 3\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0336  Progress: 32.89%  Million Words/sec: 40.90 #####\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0309  Progress: 38.26%  Million Words/sec: 41.05 #####\u001b[0m\n",
      "\u001b[31m-------------- End of epoch: 4\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0282  Progress: 43.63%  Million Words/sec: 41.15 #####\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0255  Progress: 48.92%  Million Words/sec: 41.17 #####\u001b[0m\n",
      "\u001b[31m-------------- End of epoch: 5\u001b[0m\n",
      "\u001b[31mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[31mValidation accuracy: 0.914813\u001b[0m\n",
      "\u001b[31mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0228  Progress: 54.37%  Million Words/sec: 40.19 #####\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0202  Progress: 59.67%  Million Words/sec: 40.30 #####\u001b[0m\n",
      "\u001b[31m-------------- End of epoch: 6\u001b[0m\n",
      "\u001b[31mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[31mValidation accuracy: 0.914713\u001b[0m\n",
      "\u001b[31mValidation accuracy has not improved for last 1 epochs.\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0175  Progress: 65.01%  Million Words/sec: 39.77 #####\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0148  Progress: 70.38%  Million Words/sec: 39.93 #####\u001b[0m\n",
      "\u001b[31m-------------- End of epoch: 7\u001b[0m\n",
      "\u001b[31mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[31mValidation accuracy: 0.916538\u001b[0m\n",
      "\u001b[31mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0121  Progress: 75.75%  Million Words/sec: 39.33 #####\u001b[0m\n",
      "\u001b[31m-------------- End of epoch: 8\u001b[0m\n",
      "\u001b[31mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[31mValidation accuracy: 0.91565\u001b[0m\n",
      "\u001b[31mValidation accuracy has not improved for last 1 epochs.\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0095  Progress: 81.02%  Million Words/sec: 38.99 #####\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0068  Progress: 86.31%  Million Words/sec: 39.13 #####\u001b[0m\n",
      "\u001b[31m-------------- End of epoch: 9\u001b[0m\n",
      "\u001b[31mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[31mValidation accuracy: 0.915913\u001b[0m\n",
      "\u001b[31mValidation accuracy has not improved for last 2 epochs.\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0042  Progress: 91.59%  Million Words/sec: 38.81 #####\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0016  Progress: 96.89%  Million Words/sec: 38.95 #####\u001b[0m\n",
      "\u001b[31m-------------- End of epoch: 10\u001b[0m\n",
      "\u001b[31mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[31mValidation accuracy: 0.917\u001b[0m\n",
      "\u001b[31mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0000  Progress: 100.00%  Million Words/sec: 38.67 #####\u001b[0m\n",
      "\u001b[31mTraining finished.\u001b[0m\n",
      "\u001b[31mAverage throughput in Million words/sec: 38.67\u001b[0m\n",
      "\u001b[31mTotal training time in seconds: 16.18\n",
      "\u001b[0m\n",
      "\u001b[31m#train_accuracy: 0.9768\u001b[0m\n",
      "\u001b[31mNumber of train examples: 720000\n",
      "\u001b[0m\n",
      "\u001b[31m#validation_accuracy: 0.917\u001b[0m\n",
      "\u001b[31mNumber of validation examples: 80000\u001b[0m\n",
      "\n",
      "2019-01-09 14:50:03 Uploading - Uploading generated training model\n",
      "2019-01-09 14:50:03 Completed - Training job completed\n",
      "Billable seconds: 76\n"
     ]
    }
   ],
   "source": [
    "bt_model.fit(inputs=data_channels, logs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: blazingtext-2019-01-09-14-50-42-012\n",
      "INFO:sagemaker:Creating endpoint with name Gaj-BraveSouls-2019-01-09-14-46-43-374\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------!"
     ]
    }
   ],
   "source": [
    "text_classifier = bt_model.deploy(initial_instance_count = 1,instance_type = 'ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"prob\": [\n",
      "      1.0000100135803223\n",
      "    ],\n",
      "    \"label\": [\n",
      "      \"__label__1\"\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"prob\": [\n",
      "      1.0000100135803223\n",
      "    ],\n",
      "    \"label\": [\n",
      "      \"__label__2\"\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"prob\": [\n",
      "      0.9999825954437256\n",
      "    ],\n",
      "    \"label\": [\n",
      "      \"__label__1\"\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"prob\": [\n",
      "      1.0000100135803223\n",
      "    ],\n",
      "    \"label\": [\n",
      "      \"__label__2\"\n",
      "    ]\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "sentences = [\"This product sucks\",\n",
    "            \"A very good buy. I really recommend\",\n",
    "            \"Bad design. Very hard to use\",\n",
    "            \"I'm very satisfied. The best buy ever!!\"\n",
    "    ]\n",
    "\n",
    "\n",
    "# using the same nltk tokenizer that we used during data preparation for training\n",
    "tokenized_sentences = [' '.join(nltk.word_tokenize(sent)) for sent in sentences]\n",
    "\n",
    "payload = {\"instances\" : tokenized_sentences}\n",
    "\n",
    "response = text_classifier.predict(json.dumps(payload))\n",
    "\n",
    "predictions = json.loads(response)\n",
    "print(json.dumps(predictions, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
