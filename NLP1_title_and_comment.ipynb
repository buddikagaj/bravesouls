{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Created S3 bucket: sagemaker-us-east-1-023375022819\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::023375022819:role/service-role/AmazonSageMaker-ExecutionRole-20181029T121824\n",
      "sagemaker-us-east-1-023375022819\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import json\n",
    "import boto3\n",
    "import pandas as pd\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()\n",
    "print(role) # This is the role that SageMaker would use to leverage AWS resources (S3, CloudWatch) on your behalf\n",
    "\n",
    "bucket = sess.default_bucket() # Replace with your own bucket name if needed\n",
    "print(bucket)\n",
    "prefix = 'bravesouls/supervised' #Replace with the prefix under which you want to store the data if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Data to Repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://aws-ml-chicago-team-bravesouls/amazon_review_polarity_csv.tgz to Data/amazon_review_polarity_csv.tgz\n",
      "amazon_review_polarity_csv/\n",
      "amazon_review_polarity_csv/train.csv\n",
      "amazon_review_polarity_csv/readme.txt\n",
      "amazon_review_polarity_csv/test.csv\n"
     ]
    }
   ],
   "source": [
    "!mkdir Data\n",
    "!aws s3 cp s3://aws-ml-chicago-team-bravesouls/amazon_review_polarity_csv.tgz Data\n",
    "!tar -xvzf Data/amazon_review_polarity_csv.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"amazon_review_polarity_csv/train.csv\", names=['Label', 'Title', 'Review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Combo'] = df['Title'] + ' ' + df['Review']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from random import shuffle\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "import csv\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dictionary for mapping of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_label = {}\n",
    "index_to_label['1'] = 'negative'\n",
    "index_to_label['2'] = 'positive'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_instance(row):\n",
    "    cur_row = []\n",
    "    label = \"__label__\" + index_to_label[row[0]]  #Prefix the index-ed label with __label__\n",
    "    cur_row.append(label)\n",
    "    cur_row.extend(nltk.word_tokenize(row[3].lower()))\n",
    "    return cur_row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `transform_instance` will be applied to each data instance in parallel using python's multiprocessing module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(input_file, output_file, keep=1):\n",
    "    all_rows = []\n",
    "    with open(input_file, 'r') as csvinfile:\n",
    "        csv_reader = csv.reader(csvinfile, delimiter=',')\n",
    "        for row in csv_reader:\n",
    "            row.append(row[1] + ' ' + row[2])\n",
    "            all_rows.append(row)\n",
    "    shuffle(all_rows)\n",
    "    all_rows = all_rows[:int(keep*len(all_rows))]\n",
    "    pool = Pool(processes=multiprocessing.cpu_count())\n",
    "    transformed_rows = pool.map(transform_instance, all_rows)\n",
    "    pool.close() \n",
    "    pool.join()\n",
    "    \n",
    "    with open(output_file, 'w') as csvoutfile:\n",
    "        csv_writer = csv.writer(csvoutfile, delimiter=' ', lineterminator='\\n')\n",
    "        csv_writer.writerows(transformed_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `preprocess` function will give us the test and validation sets of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 3s, sys: 8.85 s, total: 1min 12s\n",
      "Wall time: 3min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Preparing the training dataset\n",
    "\n",
    "# Since preprocessing the whole dataset might take a couple of mintutes,\n",
    "# we keep 20% of the training dataset for this demo.\n",
    "# Set keep to 1 if you want to use the complete dataset\n",
    "preprocess('amazon_review_polarity_csv/train.csv', 'amazon_review_polarity_combo1.train', keep=.2)\n",
    "        \n",
    "# Preparing the validation dataset        \n",
    "preprocess('amazon_review_polarity_csv/test.csv', 'amazon_review_polarity_combo1.validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.17 s, sys: 2.61 s, total: 5.78 s\n",
      "Wall time: 5.32 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_channel = prefix + '/train'\n",
    "validation_channel = prefix + '/validation'\n",
    "\n",
    "sess.upload_data(path='amazon_review_polarity_combo1.train', bucket=bucket, key_prefix=train_channel)\n",
    "sess.upload_data(path='amazon_review_polarity_combo1.validation', bucket=bucket, key_prefix=validation_channel)\n",
    "\n",
    "s3_train_data = 's3://{}/{}'.format(bucket, train_channel)\n",
    "s3_validation_data = 's3://{}/{}'.format(bucket, validation_channel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to setup an output location at S3, where the model artifact will be dumped. These artifacts are also the output of the algorithm's traning job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_output_location = 's3://{}/{}/output'.format(bucket, prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_name = boto3.Session().region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using SageMaker BlazingText container: 811284229777.dkr.ecr.us-east-1.amazonaws.com/blazingtext:latest (us-east-1)\n"
     ]
    }
   ],
   "source": [
    "container = sagemaker.amazon.amazon_estimator.get_image_uri(region_name, \"blazingtext\", \"latest\")\n",
    "print('Using SageMaker BlazingText container: {} ({})'.format(container, region_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "bt_model = sagemaker.estimator.Estimator(container,\n",
    "                                         role, \n",
    "                                         base_job_name= \"TM-Bravesouls\",\n",
    "                                         train_instance_count=1, \n",
    "                                         train_instance_type='ml.c4.4xlarge',\n",
    "                                         train_volume_size = 30,\n",
    "                                         train_max_run = 360000,\n",
    "                                         input_mode= 'File',\n",
    "                                         output_path=s3_output_location,\n",
    "                                         sagemaker_session=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "bt_model.set_hyperparameters(mode=\"supervised\",\n",
    "                            epochs=10,\n",
    "                            min_count=2,\n",
    "                            learning_rate=0.05,\n",
    "                            vector_dim=10,\n",
    "                            early_stopping=True,\n",
    "                            patience=4,\n",
    "                            min_epochs=5,\n",
    "                            word_ngrams=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = sagemaker.session.s3_input(s3_train_data, distribution='FullyReplicated', \n",
    "                        content_type='text/plain', s3_data_type='S3Prefix')\n",
    "validation_data = sagemaker.session.s3_input(s3_validation_data, distribution='FullyReplicated', \n",
    "                             content_type='text/plain', s3_data_type='S3Prefix')\n",
    "data_channels = {'train': train_data, 'validation': validation_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: TM-Bravesouls-2019-01-09-17-24-09-597\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-09 17:24:09 Starting - Starting the training job...\n",
      "2019-01-09 17:24:14 Starting - Launching requested ML instances......\n",
      "2019-01-09 17:25:14 Starting - Preparing the instances for training......\n",
      "2019-01-09 17:26:36 Downloading - Downloading input data\n",
      "2019-01-09 17:26:36 Training - Training image download completed. Training in progress..\n",
      "\u001b[31mArguments: train\u001b[0m\n",
      "\u001b[31m[01/09/2019 17:26:36 WARNING 139676344686400] Loggers have already been setup.\u001b[0m\n",
      "\u001b[31m[01/09/2019 17:26:36 WARNING 139676344686400] Loggers have already been setup.\u001b[0m\n",
      "\u001b[31m[01/09/2019 17:26:36 INFO 139676344686400] nvidia-smi took: 0.0251650810242 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[31m[01/09/2019 17:26:36 INFO 139676344686400] Running single machine CPU BlazingText training using supervised mode.\u001b[0m\n",
      "\u001b[31m[01/09/2019 17:26:36 INFO 139676344686400] 3 files found in train channel. Using /opt/ml/input/data/train/amazon_review_polarity_combo.train for training...\u001b[0m\n",
      "\u001b[31m[01/09/2019 17:26:36 INFO 139676344686400] Processing /opt/ml/input/data/train/amazon_review_polarity_combo.train . File size: 316 MB\u001b[0m\n",
      "\u001b[31m[01/09/2019 17:26:36 INFO 139676344686400] 3 files found in validation channel. Using /opt/ml/input/data/validation/amazon_review_polarity_combo.validation for training...\u001b[0m\n",
      "\u001b[31m[01/09/2019 17:26:36 INFO 139676344686400] Processing /opt/ml/input/data/validation/amazon_review_polarity_combo.validation . File size: 176 MB\u001b[0m\n",
      "\u001b[31mRead 10M words\u001b[0m\n",
      "\u001b[31mRead 20M words\u001b[0m\n",
      "\u001b[31mRead 30M words\u001b[0m\n",
      "\u001b[31mRead 40M words\u001b[0m\n",
      "\u001b[31mRead 50M words\u001b[0m\n",
      "\u001b[31mRead 60M words\u001b[0m\n",
      "\u001b[31mRead 66M words\u001b[0m\n",
      "\u001b[31mNumber of words:  221816\u001b[0m\n",
      "\u001b[31mLoading validation data from /opt/ml/input/data/validation/amazon_review_polarity_combo.validation\u001b[0m\n",
      "\u001b[31mLoaded validation data.\u001b[0m\n",
      "\u001b[31m-------------- End of epoch: 5\u001b[0m\n",
      "\u001b[31mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[31mValidation accuracy: 0.929785\u001b[0m\n",
      "\u001b[31mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0244  Progress: 51.30%  Million Words/sec: 37.69 #####\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0216  Progress: 56.88%  Million Words/sec: 37.99 #####\u001b[0m\n",
      "\u001b[31m-------------- End of epoch: 6\u001b[0m\n",
      "\u001b[31mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[31mValidation accuracy: 0.933098\u001b[0m\n",
      "\u001b[31mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0190  Progress: 61.95%  Million Words/sec: 36.82 #####\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0165  Progress: 67.04%  Million Words/sec: 37.17 #####\u001b[0m\n",
      "\u001b[31m-------------- End of epoch: 7\u001b[0m\n",
      "\u001b[31mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[31mValidation accuracy: 0.928503\u001b[0m\n",
      "\u001b[31mValidation accuracy has not improved for last 1 epochs.\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0140  Progress: 72.06%  Million Words/sec: 36.39 #####\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0114  Progress: 77.16%  Million Words/sec: 36.71 #####\u001b[0m\n",
      "\u001b[31m-------------- End of epoch: 8\u001b[0m\n",
      "\u001b[31mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[31mValidation accuracy: 0.932945\u001b[0m\n",
      "\u001b[31mValidation accuracy has not improved for last 2 epochs.\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0086  Progress: 82.71%  Million Words/sec: 35.93 #####\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0061  Progress: 87.80%  Million Words/sec: 36.23 #####\u001b[0m\n",
      "\u001b[31m-------------- End of epoch: 9\u001b[0m\n",
      "\u001b[31mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[31mValidation accuracy: 0.933558\u001b[0m\n",
      "\u001b[31mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0035  Progress: 93.02%  Million Words/sec: 35.63 #####\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0009  Progress: 98.11%  Million Words/sec: 35.91 #####\u001b[0m\n",
      "\u001b[31m-------------- End of epoch: 10\u001b[0m\n",
      "\u001b[31mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[31mValidation accuracy: 0.933852\u001b[0m\n",
      "\u001b[31mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0000  Progress: 100.00%  Million Words/sec: 35.39 #####\u001b[0m\n",
      "\u001b[31mTraining finished.\u001b[0m\n",
      "\u001b[31mAverage throughput in Million words/sec: 35.39\u001b[0m\n",
      "\u001b[31mTotal training time in seconds: 18.71\n",
      "\u001b[0m\n",
      "\u001b[31m#train_accuracy: 0.9826\u001b[0m\n",
      "\u001b[31mNumber of train examples: 720000\n",
      "\u001b[0m\n",
      "\u001b[31m#validation_accuracy: 0.9339\u001b[0m\n",
      "\u001b[31mNumber of validation examples: 400000\u001b[0m\n",
      "\n",
      "2019-01-09 17:27:35 Uploading - Uploading generated training model\n",
      "2019-01-09 17:27:35 Completed - Training job completed\n",
      "Billable seconds: 76\n"
     ]
    }
   ],
   "source": [
    "bt_model.fit(inputs=data_channels, logs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: blazingtext-2019-01-09-17-29-00-848\n",
      "INFO:sagemaker:Creating endpoint with name TM-Bravesouls-2019-01-09-17-24-09-597\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------!"
     ]
    }
   ],
   "source": [
    "text_classifier = bt_model.deploy(initial_instance_count = 1,instance_type = 'ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"prob\": [\n",
      "      1.0000097751617432\n",
      "    ],\n",
      "    \"label\": [\n",
      "      \"__label__positive\"\n",
      "    ]\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "            \"TCL Roku TVs are great! But... \"\n",
    "    ]\n",
    "\n",
    "\n",
    "# using the same nltk tokenizer that we used during data preparation for training\n",
    "tokenized_sentences = [' '.join(nltk.word_tokenize(sent)) for sent in sentences]\n",
    "\n",
    "payload = {\"instances\" : tokenized_sentences}\n",
    "\n",
    "response = text_classifier.predict(json.dumps(payload))\n",
    "\n",
    "predictions = json.loads(response)\n",
    "print(json.dumps(predictions, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
