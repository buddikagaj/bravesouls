{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Created S3 bucket: sagemaker-us-east-1-023375022819\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::023375022819:role/service-role/AmazonSageMaker-ExecutionRole-20181029T121824\n",
      "sagemaker-us-east-1-023375022819\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import json\n",
    "import boto3\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()\n",
    "print(role) # This is the role that SageMaker would use to leverage AWS resources (S3, CloudWatch) on your behalf\n",
    "\n",
    "bucket = sess.default_bucket() # Replace with your own bucket name if needed\n",
    "print(bucket)\n",
    "prefix = 'bravesouls/supervised/comment'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data():\n",
    "    !mkdir Data\n",
    "    !aws s3 cp s3://aws-ml-chicago-team-bravesouls/amazon_review_polarity_csv.tgz Data\n",
    "    !tar -xvzf Data/amazon_review_polarity_csv.tgz\n",
    "#prep_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!head -100000 amazon_review_polarity_csv/train.csv > amazon_review_polarity_csv/train_100k.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from random import shuffle\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "import csv\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_instance(row):\n",
    "    cur_row = []\n",
    "    label = \"__label__\" + row[0]  #Prefix the index-ed label with __label__\n",
    "    cur_row.append(label)\n",
    "    cur_row.extend(nltk.word_tokenize(row[2].lower()))\n",
    "    return cur_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(input_file, output_file, keep=1):\n",
    "    all_rows = []\n",
    "    with open(input_file, 'r') as csvinfile:\n",
    "        csv_reader = csv.reader(csvinfile, delimiter=',')\n",
    "        for row in csv_reader:\n",
    "            all_rows.append(row)\n",
    "    shuffle(all_rows)\n",
    "    all_rows = all_rows[:int(keep*len(all_rows))]\n",
    "    pool = Pool(processes=multiprocessing.cpu_count())\n",
    "    transformed_rows = pool.map(transform_instance, all_rows)\n",
    "    pool.close() \n",
    "    pool.join()\n",
    "    \n",
    "    with open(output_file, 'w') as csvoutfile:\n",
    "        csv_writer = csv.writer(csvoutfile, delimiter=' ', lineterminator='\\n')\n",
    "        csv_writer.writerows(transformed_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 46 s, sys: 5.62 s, total: 51.6 s\n",
      "Wall time: 2min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Preparing the training dataset\n",
    "\n",
    "# Since preprocessing the whole dataset might take a couple of mintutes,\n",
    "# we keep 20% of the training dataset for this demo.\n",
    "# Set keep to 1 if you want to use the complete dataset\n",
    "preprocess('amazon_review_polarity_csv/train.csv', 'amazon_review_polarity.train', keep=.2)\n",
    "        \n",
    "# Preparing the validation dataset        \n",
    "preprocess('amazon_review_polarity_csv/test.csv', 'amazon_review_polarity.validation', keep=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.32 s, sys: 1.48 s, total: 3.79 s\n",
      "Wall time: 2.23 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_channel = prefix + '/train'\n",
    "validation_channel = prefix + '/validation'\n",
    "\n",
    "sess.upload_data(path='amazon_review_polarity.train', bucket=bucket, key_prefix=train_channel)\n",
    "sess.upload_data(path='amazon_review_polarity.validation', bucket=bucket, key_prefix=validation_channel)\n",
    "\n",
    "s3_train_data = 's3://{}/{}'.format(bucket, train_channel)\n",
    "s3_validation_data = 's3://{}/{}'.format(bucket, validation_channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_output_location = 's3://{}/{}/output'.format(bucket, prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_name = boto3.Session().region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using SageMaker BlazingText container: 811284229777.dkr.ecr.us-east-1.amazonaws.com/blazingtext:latest (us-east-1)\n"
     ]
    }
   ],
   "source": [
    "container = sagemaker.amazon.amazon_estimator.get_image_uri(region_name, \"blazingtext\", \"latest\")\n",
    "print('Using SageMaker BlazingText container: {} ({})'.format(container, region_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "bt_model = sagemaker.estimator.Estimator(container,\n",
    "                                         role, \n",
    "                                         base_job_name= \"Gaj-BraveSouls\",\n",
    "                                         train_instance_count=1, \n",
    "                                         train_instance_type='ml.c4.4xlarge',\n",
    "                                         train_volume_size = 30,\n",
    "                                         train_max_run = 360000,\n",
    "                                         input_mode= 'File',\n",
    "                                         output_path=s3_output_location,\n",
    "                                         sagemaker_session=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "bt_model.set_hyperparameters(mode=\"supervised\",\n",
    "                            epochs=10,\n",
    "                            min_count=2,\n",
    "                            learning_rate=0.05,\n",
    "                            vector_dim=10,\n",
    "                            early_stopping=True,\n",
    "                            patience=4,\n",
    "                            min_epochs=5,\n",
    "                            word_ngrams=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = sagemaker.session.s3_input(s3_train_data, distribution='FullyReplicated', \n",
    "                        content_type='text/plain', s3_data_type='S3Prefix')\n",
    "validation_data = sagemaker.session.s3_input(s3_validation_data, distribution='FullyReplicated', \n",
    "                             content_type='text/plain', s3_data_type='S3Prefix')\n",
    "data_channels = {'train': train_data, 'validation': validation_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: Gaj-BraveSouls-2019-01-09-16-26-45-769\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-09 16:26:45 Starting - Starting the training job...\n",
      "2019-01-09 16:26:51 Starting - Launching requested ML instances.........\n",
      "2019-01-09 16:28:23 Starting - Preparing the instances for training......\n",
      "2019-01-09 16:29:36 Downloading - Downloading input data\n",
      "2019-01-09 16:29:36 Training - Training image download completed. Training in progress..\n",
      "\u001b[31mArguments: train\u001b[0m\n",
      "\u001b[31m[01/09/2019 16:29:37 WARNING 140146227361600] Loggers have already been setup.\u001b[0m\n",
      "\u001b[31m[01/09/2019 16:29:37 WARNING 140146227361600] Loggers have already been setup.\u001b[0m\n",
      "\u001b[31m[01/09/2019 16:29:37 INFO 140146227361600] nvidia-smi took: 0.0251801013947 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[31m[01/09/2019 16:29:37 INFO 140146227361600] Running single machine CPU BlazingText training using supervised mode.\u001b[0m\n",
      "\u001b[31m[01/09/2019 16:29:37 INFO 140146227361600] Processing /opt/ml/input/data/train/amazon_review_polarity.train . File size: 294 MB\u001b[0m\n",
      "\u001b[31m[01/09/2019 16:29:37 INFO 140146227361600] Processing /opt/ml/input/data/validation/amazon_review_polarity.validation . File size: 32 MB\u001b[0m\n",
      "\u001b[31mRead 10M words\u001b[0m\n",
      "\u001b[31mRead 20M words\u001b[0m\n",
      "\u001b[31mRead 30M words\u001b[0m\n",
      "\u001b[31mRead 40M words\u001b[0m\n",
      "\u001b[31mRead 50M words\u001b[0m\n",
      "\u001b[31mRead 60M words\u001b[0m\n",
      "\u001b[31mRead 62M words\u001b[0m\n",
      "\u001b[31mNumber of words:  210985\u001b[0m\n",
      "\u001b[31mLoading validation data from /opt/ml/input/data/validation/amazon_review_polarity.validation\u001b[0m\n",
      "\u001b[31mLoaded validation data.\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0448  Progress: 10.47%  Million Words/sec: 39.35 #####\u001b[0m\n",
      "\u001b[31m-------------- End of epoch: 1\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0421  Progress: 15.74%  Million Words/sec: 39.95 #####\u001b[0m\n",
      "\u001b[31m-------------- End of epoch: 2\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0395  Progress: 21.07%  Million Words/sec: 40.38 #####\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0368  Progress: 26.34%  Million Words/sec: 40.54 #####\u001b[0m\n",
      "\u001b[31m-------------- End of epoch: 3\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0342  Progress: 31.66%  Million Words/sec: 40.70 #####\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0315  Progress: 36.99%  Million Words/sec: 40.85 #####\u001b[0m\n",
      "\u001b[31m-------------- End of epoch: 4\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0288  Progress: 42.31%  Million Words/sec: 40.90 #####\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0262  Progress: 47.63%  Million Words/sec: 40.98 #####\u001b[0m\n",
      "\u001b[31m-------------- End of epoch: 5\u001b[0m\n",
      "\u001b[31mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[31mValidation accuracy: 0.912887\u001b[0m\n",
      "\u001b[31mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0235  Progress: 52.97%  Million Words/sec: 39.89 #####\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0208  Progress: 58.30%  Million Words/sec: 40.05 #####\u001b[0m\n",
      "\u001b[31m-------------- End of epoch: 6\u001b[0m\n",
      "\u001b[31mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[31mValidation accuracy: 0.911887\u001b[0m\n",
      "\u001b[31mValidation accuracy has not improved for last 1 epochs.\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0182  Progress: 63.58%  Million Words/sec: 39.51 #####\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0156  Progress: 68.85%  Million Words/sec: 39.63 #####\u001b[0m\n",
      "\u001b[31m-------------- End of epoch: 7\u001b[0m\n",
      "\u001b[31mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[31mValidation accuracy: 0.913563\u001b[0m\n",
      "\u001b[31mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0129  Progress: 74.22%  Million Words/sec: 39.09 #####\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0102  Progress: 79.50%  Million Words/sec: 39.23 #####\u001b[0m\n",
      "\u001b[31m-------------- End of epoch: 8\u001b[0m\n",
      "\u001b[31mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[31mValidation accuracy: 0.914925\u001b[0m\n",
      "\u001b[31mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0076  Progress: 84.82%  Million Words/sec: 38.75 #####\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0049  Progress: 90.15%  Million Words/sec: 38.91 #####\u001b[0m\n",
      "\u001b[31m-------------- End of epoch: 9\u001b[0m\n",
      "\u001b[31mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[31mValidation accuracy: 0.915475\u001b[0m\n",
      "\u001b[31mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0023  Progress: 95.45%  Million Words/sec: 38.52 #####\u001b[0m\n",
      "\u001b[31m-------------- End of epoch: 10\u001b[0m\n",
      "\u001b[31mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[31mValidation accuracy: 0.916288\u001b[0m\n",
      "\u001b[31mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0000  Progress: 100.00%  Million Words/sec: 38.35 #####\u001b[0m\n",
      "\n",
      "2019-01-09 16:30:16 Uploading - Uploading generated training model\u001b[31mTraining finished.\u001b[0m\n",
      "\u001b[31mAverage throughput in Million words/sec: 38.35\u001b[0m\n",
      "\u001b[31mTotal training time in seconds: 16.34\n",
      "\u001b[0m\n",
      "\u001b[31m#train_accuracy: 0.977\u001b[0m\n",
      "\u001b[31mNumber of train examples: 720000\n",
      "\u001b[0m\n",
      "\u001b[31m#validation_accuracy: 0.9163\u001b[0m\n",
      "\u001b[31mNumber of validation examples: 80000\u001b[0m\n",
      "\n",
      "2019-01-09 16:30:32 Completed - Training job completed\n",
      "Billable seconds: 75\n"
     ]
    }
   ],
   "source": [
    "bt_model.fit(inputs=data_channels, logs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: blazingtext-2019-01-09-16-40-19-790\n",
      "INFO:sagemaker:Creating endpoint with name Gaj-BraveSouls-2019-01-09-16-26-45-769\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------!"
     ]
    }
   ],
   "source": [
    "text_classifier = bt_model.deploy(initial_instance_count = 1,instance_type = 'ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"prob\": [\n",
      "      1.0000100135803223\n",
      "    ],\n",
      "    \"label\": [\n",
      "      \"__label__1\"\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"prob\": [\n",
      "      1.0000100135803223\n",
      "    ],\n",
      "    \"label\": [\n",
      "      \"__label__2\"\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"prob\": [\n",
      "      0.9988150596618652\n",
      "    ],\n",
      "    \"label\": [\n",
      "      \"__label__1\"\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"prob\": [\n",
      "      1.0000100135803223\n",
      "    ],\n",
      "    \"label\": [\n",
      "      \"__label__2\"\n",
      "    ]\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "sentences = [\"This product sucks\",\n",
    "            \"A very good buy. I really recommend\",\n",
    "            \"Bad design. Very hard to use\",\n",
    "            \"I'm very satisfied. The best buy ever!!\"\n",
    "    ]\n",
    "\n",
    "\n",
    "# using the same nltk tokenizer that we used during data preparation for training\n",
    "tokenized_sentences = [' '.join(nltk.word_tokenize(sent)) for sent in sentences]\n",
    "\n",
    "payload = {\"instances\" : tokenized_sentences}\n",
    "\n",
    "response = text_classifier.predict(json.dumps(payload))\n",
    "\n",
    "predictions = json.loads(response)\n",
    "print(json.dumps(predictions, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
