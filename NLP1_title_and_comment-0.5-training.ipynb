{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::023375022819:role/service-role/AmazonSageMaker-ExecutionRole-20181029T121824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Created S3 bucket: sagemaker-us-east-1-023375022819\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker-us-east-1-023375022819\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import json\n",
    "import boto3\n",
    "import pandas as pd\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()\n",
    "print(role) # This is the role that SageMaker would use to leverage AWS resources (S3, CloudWatch) on your behalf\n",
    "\n",
    "bucket = sess.default_bucket() # Replace with your own bucket name if needed\n",
    "print(bucket)\n",
    "prefix = 'bravesouls/supervised' #Replace with the prefix under which you want to store the data if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Data to Repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir Data\n",
    "!aws s3 cp s3://aws-ml-chicago-team-bravesouls/amazon_review_polarity_csv.tgz Data\n",
    "!tar -xvzf Data/amazon_review_polarity_csv.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"amazon_review_polarity_csv/train.csv\", names=['Label', 'Title', 'Review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Combo'] = df['Title'] + ' ' + df['Review']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from random import shuffle\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "import csv\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dictionary for mapping of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_label = {}\n",
    "index_to_label['1'] = 'negative'\n",
    "index_to_label['2'] = 'positive'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_instance(row):\n",
    "    cur_row = []\n",
    "    label = \"__label__\" + index_to_label[row[0]]  #Prefix the index-ed label with __label__\n",
    "    cur_row.append(label)\n",
    "    cur_row.extend(nltk.word_tokenize(row[1].lower() + ' ' + row[2].lower()))\n",
    "    return cur_row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `transform_instance` will be applied to each data instance in parallel using python's multiprocessing module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(input_file, output_file, keep=1):\n",
    "    all_rows = []\n",
    "    with open(input_file, 'r') as csvinfile:\n",
    "        csv_reader = csv.reader(csvinfile, delimiter=',')\n",
    "        for row in csv_reader:\n",
    "            all_rows.append(row)\n",
    "    shuffle(all_rows)\n",
    "    all_rows = all_rows[:int(keep*len(all_rows))]\n",
    "    pool = Pool(processes=multiprocessing.cpu_count())\n",
    "    transformed_rows = pool.map(transform_instance, all_rows)\n",
    "    pool.close() \n",
    "    pool.join()\n",
    "    \n",
    "    with open(output_file, 'w') as csvoutfile:\n",
    "        csv_writer = csv.writer(csvoutfile, delimiter=' ', lineterminator='\\n')\n",
    "        csv_writer.writerows(transformed_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `preprocess` function will give us the test and validation sets of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 30s, sys: 13.6 s, total: 1min 43s\n",
      "Wall time: 6min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Preparing the training dataset\n",
    "\n",
    "# Since preprocessing the whole dataset might take a couple of mintutes,\n",
    "# we keep 20% of the training dataset for this demo.\n",
    "# Set keep to 1 if you want to use the complete dataset\n",
    "preprocess('amazon_review_polarity_csv/train.csv', 'amazon_review_polarity_combo3.train', keep=.5)\n",
    "        \n",
    "# Preparing the validation dataset        \n",
    "preprocess('amazon_review_polarity_csv/test.csv', 'amazon_review_polarity_combo3.validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.12 s, sys: 3.71 s, total: 8.82 s\n",
      "Wall time: 8.59 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_channel = prefix + '/train'\n",
    "validation_channel = prefix + '/validation'\n",
    "\n",
    "sess.upload_data(path='amazon_review_polarity_combo3.train', bucket=bucket, key_prefix=train_channel)\n",
    "sess.upload_data(path='amazon_review_polarity_combo3.validation', bucket=bucket, key_prefix=validation_channel)\n",
    "\n",
    "s3_train_data = 's3://{}/{}'.format(bucket, train_channel)\n",
    "s3_validation_data = 's3://{}/{}'.format(bucket, validation_channel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to setup an output location at S3, where the model artifact will be dumped. These artifacts are also the output of the algorithm's traning job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_output_location = 's3://{}/{}/output'.format(bucket, prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_name = boto3.Session().region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using SageMaker BlazingText container: 811284229777.dkr.ecr.us-east-1.amazonaws.com/blazingtext:latest (us-east-1)\n"
     ]
    }
   ],
   "source": [
    "container = sagemaker.amazon.amazon_estimator.get_image_uri(region_name, \"blazingtext\", \"latest\")\n",
    "print('Using SageMaker BlazingText container: {} ({})'.format(container, region_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bt_model = sagemaker.estimator.Estimator(container,\n",
    "                                         role, \n",
    "                                         base_job_name= \"TM-Bravesouls-half\",\n",
    "                                         train_instance_count=1, \n",
    "                                         train_instance_type='ml.c4.4xlarge',\n",
    "                                         train_volume_size = 30,\n",
    "                                         train_max_run = 360000,\n",
    "                                         input_mode= 'File',\n",
    "                                         output_path=s3_output_location,\n",
    "                                         sagemaker_session=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "bt_model.set_hyperparameters(mode=\"supervised\",\n",
    "                            epochs=10,\n",
    "                            min_count=2,\n",
    "                            learning_rate=0.05,\n",
    "                            vector_dim=10,\n",
    "                            early_stopping=True,\n",
    "                            patience=4,\n",
    "                            min_epochs=5,\n",
    "                            word_ngrams=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = sagemaker.session.s3_input(s3_train_data, distribution='FullyReplicated', \n",
    "                        content_type='text/plain', s3_data_type='S3Prefix')\n",
    "\n",
    "validation_data = sagemaker.session.s3_input(s3_validation_data, distribution='FullyReplicated', \n",
    "                              content_type='text/plain', s3_data_type='S3Prefix')\n",
    "\n",
    "data_channels = {'train': train_data, 'validation': validation_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: TM-Bravesouls-half-2019-01-09-22-54-10-108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-09 22:54:10 Starting - Starting the training job...\n",
      "2019-01-09 22:54:22 Starting - Launching requested ML instances......\n",
      "2019-01-09 22:55:27 Starting - Preparing the instances for training...\n",
      "2019-01-09 22:56:06 Downloading - Downloading input data.....\n",
      "\u001b[31mArguments: train\u001b[0m\n",
      "\u001b[31m[01/09/2019 22:56:45 WARNING 140572560639808] Loggers have already been setup.\u001b[0m\n",
      "\u001b[31m[01/09/2019 22:56:45 WARNING 140572560639808] Loggers have already been setup.\u001b[0m\n",
      "\u001b[31m[01/09/2019 22:56:45 INFO 140572560639808] nvidia-smi took: 0.0251631736755 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[31m[01/09/2019 22:56:45 INFO 140572560639808] Running single machine CPU BlazingText training using supervised mode.\u001b[0m\n",
      "\u001b[31m[01/09/2019 22:56:45 INFO 140572560639808] 5 files found in train channel. Using /opt/ml/input/data/train/amazon_review_polarity_combo3.train for training...\u001b[0m\n",
      "\u001b[31m[01/09/2019 22:56:45 INFO 140572560639808] Processing /opt/ml/input/data/train/amazon_review_polarity_combo3.train . File size: 793 MB\u001b[0m\n",
      "\u001b[31m[01/09/2019 22:56:45 INFO 140572560639808] 5 files found in validation channel. Using /opt/ml/input/data/validation/amazon_review_polarity_combo1.validation for training...\u001b[0m\n",
      "\u001b[31m[01/09/2019 22:56:45 INFO 140572560639808] Processing /opt/ml/input/data/validation/amazon_review_polarity_combo1.validation . File size: 176 MB\u001b[0m\n",
      "\u001b[31mRead 10M words\u001b[0m\n",
      "\u001b[31mRead 20M words\u001b[0m\n",
      "\u001b[31mRead 30M words\u001b[0m\n",
      "\u001b[31mRead 40M words\u001b[0m\n",
      "\u001b[31mRead 50M words\u001b[0m\n",
      "\u001b[31mRead 60M words\u001b[0m\n",
      "\u001b[31mRead 70M words\u001b[0m\n",
      "\u001b[31mRead 80M words\u001b[0m\n",
      "\u001b[31mRead 90M words\u001b[0m\n",
      "\u001b[31mRead 100M words\u001b[0m\n",
      "\u001b[31mRead 110M words\u001b[0m\n",
      "\n",
      "2019-01-09 22:56:45 Training - Training image download completed. Training in progress.\u001b[31mRead 120M words\u001b[0m\n",
      "\u001b[31mRead 130M words\u001b[0m\n",
      "\u001b[31mRead 140M words\u001b[0m\n",
      "\u001b[31mRead 150M words\u001b[0m\n",
      "\u001b[31mRead 160M words\u001b[0m\n",
      "\u001b[31mRead 165M words\u001b[0m\n",
      "\u001b[31mNumber of words:  406763\u001b[0m\n",
      "\u001b[31mLoading validation data from /opt/ml/input/data/validation/amazon_review_polarity_combo1.validation\u001b[0m\n",
      "\u001b[31mLoaded validation data.\u001b[0m\n",
      "\u001b[31m-------------- End of epoch: 1\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0399  Progress: 20.25%  Million Words/sec: 39.37 #####\u001b[0m\n",
      "\u001b[31m-------------- End of epoch: 2\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0373  Progress: 25.30%  Million Words/sec: 39.84 #####\u001b[0m\n",
      "\u001b[31m-------------- End of epoch: 3\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0348  Progress: 30.36%  Million Words/sec: 40.16 #####\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0323  Progress: 35.42%  Million Words/sec: 40.39 #####\u001b[0m\n",
      "\u001b[31m-------------- End of epoch: 4\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0298  Progress: 40.47%  Million Words/sec: 40.56 #####\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0272  Progress: 45.52%  Million Words/sec: 40.69 #####\u001b[0m\n",
      "\u001b[31m-------------- End of epoch: 5\u001b[0m\n",
      "\u001b[31mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[31mValidation accuracy: 0.93646\u001b[0m\n",
      "\u001b[31mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0247  Progress: 50.66%  Million Words/sec: 39.82 #####\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0221  Progress: 55.71%  Million Words/sec: 39.99 #####\u001b[0m\n",
      "\u001b[31m-------------- End of epoch: 6\u001b[0m\n",
      "\u001b[31mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[31mValidation accuracy: 0.937778\u001b[0m\n",
      "\u001b[31mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0196  Progress: 60.83%  Million Words/sec: 39.49 #####\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0171  Progress: 65.87%  Million Words/sec: 39.66 #####\u001b[0m\n",
      "\u001b[31m-------------- End of epoch: 7\u001b[0m\n",
      "\u001b[31mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[31mValidation accuracy: 0.937625\u001b[0m\n",
      "\u001b[31mValidation accuracy has not improved for last 1 epochs.\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0145  Progress: 70.96%  Million Words/sec: 39.29 #####\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0120  Progress: 76.01%  Million Words/sec: 39.45 #####\u001b[0m\n",
      "\u001b[31m-------------- End of epoch: 8\u001b[0m\n",
      "\u001b[31mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[31mValidation accuracy: 0.93702\u001b[0m\n",
      "\u001b[31mValidation accuracy has not improved for last 2 epochs.\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0095  Progress: 81.03%  Million Words/sec: 39.17 #####\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0070  Progress: 86.08%  Million Words/sec: 39.32 #####\u001b[0m\n",
      "\u001b[31m-------------- End of epoch: 9\u001b[0m\n",
      "\u001b[31mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[31mValidation accuracy: 0.936645\u001b[0m\n",
      "\u001b[31mValidation accuracy has not improved for last 3 epochs.\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0045  Progress: 91.10%  Million Words/sec: 38.97 #####\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0019  Progress: 96.16%  Million Words/sec: 39.11 #####\u001b[0m\n",
      "\u001b[31m-------------- End of epoch: 10\u001b[0m\n",
      "\u001b[31mUsing 16 threads for prediction!\u001b[0m\n",
      "\u001b[31mValidation accuracy: 0.93864\u001b[0m\n",
      "\u001b[31mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[31m##### Alpha: 0.0000  Progress: 100.00%  Million Words/sec: 38.84 #####\u001b[0m\n",
      "\u001b[31mTraining finished.\u001b[0m\n",
      "\u001b[31mAverage throughput in Million words/sec: 38.84\u001b[0m\n",
      "\u001b[31mTotal training time in seconds: 42.67\n",
      "\u001b[0m\n",
      "\u001b[31m#train_accuracy: 0.9784\u001b[0m\n",
      "\u001b[31mNumber of train examples: 1800000\n",
      "\u001b[0m\n",
      "\u001b[31m#validation_accuracy: 0.9386\u001b[0m\n",
      "\u001b[31mNumber of validation examples: 400000\u001b[0m\n",
      "\n",
      "2019-01-09 22:58:35 Uploading - Uploading generated training model\n",
      "2019-01-09 22:58:35 Completed - Training job completed\n",
      "Billable seconds: 149\n"
     ]
    }
   ],
   "source": [
    "bt_model.fit(inputs=data_channels, logs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: blazingtext-2019-01-09-17-29-00-848\n",
      "INFO:sagemaker:Creating endpoint with name TM-Bravesouls-2019-01-09-17-24-09-597\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------!"
     ]
    }
   ],
   "source": [
    "text_classifier = bt_model.deploy(initial_instance_count = 1,instance_type = 'ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"prob\": [\n",
      "      1.0000097751617432\n",
      "    ],\n",
      "    \"label\": [\n",
      "      \"__label__positive\"\n",
      "    ]\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "            \"TCL Roku TVs are great! But... \"\n",
    "    ]\n",
    "\n",
    "\n",
    "# using the same nltk tokenizer that we used during data preparation for training\n",
    "tokenized_sentences = [' '.join(nltk.word_tokenize(sent)) for sent in sentences]\n",
    "\n",
    "payload = {\"instances\" : tokenized_sentences}\n",
    "\n",
    "response = text_classifier.predict(json.dumps(payload))\n",
    "\n",
    "predictions = json.loads(response)\n",
    "print(json.dumps(predictions, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
